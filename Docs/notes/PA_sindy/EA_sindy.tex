\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{accents}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{centernot}
\newcommand{\bs}{{\bigskip}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceiling}[1]{\lceil #1 \rceil}
\newcommand{\ms}{{\medskip}}
\newcommand{\sms}{{\smallskip}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\indi}{\mathbbm{1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\sigF}{\mathcal{F}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\dd}[2]{\frac{d{#1}}{d{#2}}}
\newcommand{\fancyF}{\mathscr{F}}
\newcommand{\borelB}{\mathcal{B}}
\newcommand{\Var}{\text{Var}}
\newcommand{\inner}[2]{\langle{#1},{#2}\rangle}
\newcommand{\sinner}[1]{\langle{#1},{#1} \rangle}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\overrightarrow}{\vec}
\newcommand{\tb}{\textbf}
\newcommand{\bfrac}[2]{\displaystyle{\frac{#1}{#2}}}
\newcommand{\bcup}{\bigcup\limits}
\newcommand{\bcap}{\bigcap\limits}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\nimply}{\centernot\Rightarrow}
\newcommand{\ar}{\Rightarrow}
\newcommand{\norm}[2]{\| #1 \|_{#2}}
\newcommand{\bnorm}[1]{\norm{#1}{}}
\newcommand{\probp}{\mathbb{P}}
\newcommand{\restrict}[2]{#1\big{|}_{#2}}

\newcommand{\enorm}[1]{\| #1 \|}
\newcommand{\goto}{\rightarrow}
\newcommand{\bint}[2]{\displaystyle{\int_{#1}^{#2}}}
\newcommand{\nogoto}{\centernot\rightarrow}
\renewcommand{\baselinestretch}{1.5}
\newcommand{\bsum}[2]{\displaystyle{\sum_{#1}^{#2}}}
\newcommand{\bprod}[2]{\displaystyle{\prod_{#1}^{#2}}}
\newcommand{\func}[3]{#1: #2\rightarrow#3}
\newcommand{\sfunc}[2]{#1: #2\rightarrow#2}
\newcommand{\cexp}[2]{\E[#1 \mid #2]}
\usepackage{venndiagram}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\Limn}{\Lim{n \in \N}}
\newcommand{\Cross}{\mathbin{\tikz [x=1.4ex,y=1.4ex,line width=.2ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}
\title{ EA-sindy}
\author{Alois D'uston,  ald6fd}
\setlength{\parindent}{0pt}

\begin{document}


\maketitle


\tb{Model defenition}:

Base Autoencoder Sindy begins with data matrixes $X,  \dot{X} \in \R^{m \times D}$. 

Each column $X_i \sim \mathcal{X}$.  Derivatives $\dot{X}_i$ in  $\dot{X}$ are computed numerically.  
 
We fit model $\varphi^{-1}(\Theta(\varphi(x))\Xi) \approx \dot{x}$  using training data $X,  \dot{X}$ , where:


i.  $\func{\varphi}{\R^D}{\R^d}$, $\func{\varphi^{-1}}{\R^d}{\R^D}$ are encoder decoder pair s.t  
$\restrict{(\varphi^{-1} \circ \varphi)}{\mathcal{X}} \approx  I$

ii.  $\func{\Theta}{\R^d}{\R^p}$ evaluates function library(consisting of $p$ funcs) on $\varphi(x)$.

iii.  $\Xi \in \R^{d \times p}$ are linear coefficients associated to each term in ${\Theta}(\varphi(x))$.

We fit this model using gradient descent with loss function:

$\mathcal{L}(\varphi, \varphi^{-1},  \Xi ; x ,\dot{x}) = \mathcal{L}_\text{encode}(\varphi, \varphi^{-1}; x) + \mathcal{L}_\text{fit}(\varphi, \varphi^{-1},\Xi; x,\dot{x}) + \mathcal{L}_\text{reg}(\Xi)$

 $\mathcal{L}_\text{encode}$ enforces that $\restrict{(\varphi^{-1} \circ \varphi)}{\mathcal{X}} \approx  I$
 
 $\mathcal{L}_\text{fit}$ enforces that $\varphi^{-1}(\Theta(\varphi(x))\Xi) \approx \dot{x}$

 $\mathcal{L}_\text{reg}$ promotes sparsity in  coefficients $\Xi$.
 Usually   use $\mathcal{L}_\text{reg}(\Xi) = \lambda_\text{reg}\norm{\Xi}{1}$
 
 $L_1$ regularization results in $\Xi$ matrix where lots of coefficients are close to zero.  Our prior assumption is that only a few coefficients are nonzero. 
 
  We use coefficient mask $\Lambda$ to enforce coherence to this assumption.  
 
 At epoch $k$ of training we set $\Lambda ^{(k)}_{ij} = \indi\{(\Lambda ^{(k-1)}\odot \Xi^{(k-1)})_{ij} \approx 0\}$.
 
 \pagebreak
 
 
 In ensemble autoencoder sindy we split our training data $X,\dot{X}$ 	into $b$ bags $\{X^{(i)},  \dot{X}^{(i)}\}_{i=1}^b$ where $X^{(i)},\dot{X}^{(i)} \in \R^{q \times D}$ are sampled from the training 
 
 samples $X,\dot{X}$ with replacement. 
  We consider coefficient tensor 
  
  $\Xi^{[1:b]} \in \R^{b \times d \times p}$,  where $\Xi^{[1:b]}_{[i,:,:]} = \Xi^{(i)}$ 
 corresponds to bag $X^{(i)},\dot{X}^{(i)}$ of the data.  In training,  we fit model
   $\varphi^{-1}(\Theta(\varphi(x))\sum_{i} \Xi^{(i)}\indi\{x \in X^{(i)}\}) \approx \dot{x}$.
 
 
 We do so via gradient descent using loss function:
 
 $\mathcal{L}(\varphi, \varphi^{-1}, \Xi^{[1:b]} ; x ,\dot{x}) = \mathcal{L}_\text{encode}(\varphi, \varphi^{-1}; x) + \mathcal{L}_\text{fit}(\varphi, \varphi^{-1},\Xi^{[1:b]}; x,\dot{x}) + \mathcal{L}_\text{reg}(\Xi^{[1:b]})$
 
  $\mathcal{L}_\text{encode}$ is unchanged, enforces that $\restrict{(\varphi^{-1} \circ \varphi)}{\mathcal{X}} \approx  I$
 
 
  $\mathcal{L}_\text{fit}$ enforces that    $\varphi^{-1}(\Theta(\varphi(x))\sum_{i} \Xi^{(i)}\indi\{x \in X^{(i)}\}) \approx \dot{x}$.
  
  
   $\mathcal{L}_\text{reg}$ promotes sparsity in the average of coefficients  $\Xi^{(1)},...\Xi^{(n)}$
   
    Ie it promotes sparsity in   $\tilde{\Xi} = \frac{1}{b}\sum_{i} \Xi^{(i)}$.
     We use $\mathcal{L}_\text{reg}(\Xi^{[1:b]}) = \lambda_\text{reg}\norm{\frac{1}{b}\sum_{i} \Xi^{(i)}}{1}$
 
 
 The final output of the model, used in testing,  is the avg coefficient matrix  $\tilde{\Xi} = \frac{1}{b}\sum_{i} \Xi^{(i)}$.  We again use a coefficient mask $\Lambda$ to enforce our prior assumption that only a few coefficients of $\tilde{\Xi}$ are nonzero. 
 At epoch $k$ of training we set $\Lambda ^{(k)}_{ij} = \indi\{\frac{1}{b}
  | \{h : (\Lambda ^{(k-1)}\odot \Xi^{(h),(k-1)})_{ij} \approx 0\}| > p_\text{tol} \}$.
  
  This procedure is known is stability selection.  It works by finding  all those coefficients $\Xi_{ij}$ which aren't consistently activated(well seperated from 0) across data bags $\{X^{(i)},  \dot{X}^{(i)}\}_{i=1}^b$,   and setting these coefficients to $0$.
  
  \pagebreak
 
 \tb{Results}:
 
 
 \pagebreak
 
 
 \tb{Research directions}:
  
  
  
  \pagebreak
  
  
  \tb{Implementation details}:
  
 \pagebreak
 
  




\end{document}